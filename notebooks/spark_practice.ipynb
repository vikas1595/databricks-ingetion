{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing warehouse directory: ./temp/warehouse/\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), \"..\")))\n",
    "from src.spark_session import local_spark_session\n",
    "spark = local_spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 'HR', 77625)\n",
      "(2, 'Marketing', 57035)\n",
      "(3, 'Marketing', 108216)\n",
      "(4, 'Marketing', 83747)\n",
      "(5, 'Finance', 92952)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "# Define sample departments\n",
    "departments = [\"HR\", \"IT\", \"Finance\", \"Marketing\"]\n",
    "\n",
    "# Generate sample employee data as a list of dictionaries\n",
    "row_count = 100\n",
    "employee_data = [\n",
    "    (\n",
    "         i + 1,  # Unique employee ID\n",
    "        random.choice(departments),  # Random department\n",
    "         random.randint(30000, 120000)  # Random salary\n",
    "    )\n",
    "    for i in range(row_count)\n",
    "]\n",
    "\n",
    "# Print first 5 records\n",
    "for record in employee_data[:5]:\n",
    "    print(record)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the salary of employees with the average salary within their department "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+-----------------+-----------+\n",
      "|emp_id|department|salary|       avg_salary|comparision|\n",
      "+------+----------+------+-----------------+-----------+\n",
      "|     5|   Finance| 92952|74120.44444444444|     Higher|\n",
      "|     6|   Finance| 76872|74120.44444444444|     Higher|\n",
      "|     9|   Finance| 82601|74120.44444444444|     Higher|\n",
      "|    11|   Finance| 37446|74120.44444444444|      lower|\n",
      "|    16|   Finance| 65255|74120.44444444444|      lower|\n",
      "|    24|   Finance| 69857|74120.44444444444|      lower|\n",
      "|    25|   Finance|112849|74120.44444444444|     Higher|\n",
      "|    27|   Finance| 77618|74120.44444444444|     Higher|\n",
      "|    30|   Finance| 70417|74120.44444444444|      lower|\n",
      "|    34|   Finance|109041|74120.44444444444|     Higher|\n",
      "|    42|   Finance| 52419|74120.44444444444|      lower|\n",
      "|    45|   Finance| 94804|74120.44444444444|     Higher|\n",
      "|    47|   Finance| 56671|74120.44444444444|      lower|\n",
      "|    52|   Finance| 99107|74120.44444444444|     Higher|\n",
      "|    72|   Finance| 74527|74120.44444444444|     Higher|\n",
      "|    73|   Finance| 51924|74120.44444444444|      lower|\n",
      "|    75|   Finance|101450|74120.44444444444|     Higher|\n",
      "|    76|   Finance| 46184|74120.44444444444|      lower|\n",
      "|    79|   Finance| 71154|74120.44444444444|      lower|\n",
      "|    90|   Finance| 48230|74120.44444444444|      lower|\n",
      "+------+----------+------+-----------------+-----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import avg, col, when\n",
    "from pyspark.sql.types import StructType,StructField,IntegerType,StringType\n",
    "# create struct schema\n",
    "expected_schema= (StructType().add(StructField(\"emp_id\",IntegerType()))\n",
    "                  .add(StructField('department',StringType()))\n",
    "                  .add(StructField('salary',IntegerType())))\n",
    "employee_df=spark.createDataFrame(employee_data,expected_schema)\n",
    "# employee_df.show()\n",
    "\n",
    "avg_emp_salary = employee_df.withColumn(\n",
    "    \"avg_salary\", avg(col(\"salary\")).over(Window.partitionBy(col(\"department\")))\n",
    ")\n",
    "comparision_df = avg_emp_salary.withColumn(\n",
    "    \"comparision\",\n",
    "    when(col(\"salary\") > col(\"avg_salary\"), \"Higher\")\n",
    "    .when(col(\"salary\").eqNullSafe(col(\"avg_salary\")), \"equal\")\n",
    "    .otherwise(\"lower\"),\n",
    ")\n",
    "comparision_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# nth highest salary for each department"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|emp_id|department|salary|\n",
      "+------+----------+------+\n",
      "|    96|   Finance|116959|\n",
      "|    56|        HR|117795|\n",
      "|    67|        IT|111359|\n",
      "|    85| Marketing|119723|\n",
      "+------+----------+------+\n",
      "\n",
      "+------+----------+------+\n",
      "|emp_id|department|salary|\n",
      "+------+----------+------+\n",
      "|    25|   Finance|112849|\n",
      "|    50|        HR|117185|\n",
      "|    55|        IT|108593|\n",
      "|    71| Marketing|119598|\n",
      "+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "from pyspark.sql.functions import desc, asc, dense_rank, lit\n",
    "from pyspark.sql.dataframe import DataFrame\n",
    "\n",
    "# dept wide window\n",
    "dept_window = Window.partitionBy(col(\"department\")).orderBy([desc(col(\"salary\"))])\n",
    "\n",
    "\n",
    "def nth_highest_salary(spark, input_df: DataFrame, rank: int) -> DataFrame:\n",
    "    df = input_df.withColumn(\"rank\", dense_rank().over(dept_window))\n",
    "    filtered_df = df.filter(col(\"rank\") == lit(rank)).select(\n",
    "        \"emp_id\", \"department\", \"salary\"\n",
    "    )\n",
    "    return filtered_df\n",
    "\n",
    "\n",
    "df = nth_highest_salary(spark, employee_df, 1)\n",
    "df.show()\n",
    "df = nth_highest_salary(spark, employee_df, 2)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Write a Pyspark code, to filter the salary between 20000 and 30000.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown\n",
    "# Write a Pyspark code, to filter the salary between 20000 and 30000.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+------+\n",
      "|emp_id|department|salary|\n",
      "+------+----------+------+\n",
      "|     1|        HR| 77625|\n",
      "|     6|   Finance| 76872|\n",
      "|     8|        IT| 79448|\n",
      "|    27|   Finance| 77618|\n",
      "|    30|   Finance| 70417|\n",
      "|    32|        HR| 72176|\n",
      "|    33|        IT| 70707|\n",
      "|    36|        IT| 76538|\n",
      "|    69| Marketing| 72629|\n",
      "|    72|   Finance| 74527|\n",
      "|    79|   Finance| 71154|\n",
      "|    88|        IT| 71422|\n",
      "|    91|   Finance| 77029|\n",
      "|    95| Marketing| 75785|\n",
      "|    98|   Finance| 72506|\n",
      "+------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "employee_df.filter(col('salary').between(lit(70000),lit(80000))).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# count number of null values\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%markdown \n",
    "# count number of null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+-------------------+---------------+\n",
      "|null_cnt_emp_id|null_cnt_emp_name|null_cnt_department|null_cnt_salary|\n",
      "+---------------+-----------------+-------------------+---------------+\n",
      "|              0|                2|                  2|              3|\n",
      "+---------------+-----------------+-------------------+---------------+\n",
      "\n",
      "+------+--------+----------+------+\n",
      "|emp_id|emp_name|department|salary|\n",
      "+------+--------+----------+------+\n",
      "|     0|       2|         2|     3|\n",
      "+------+--------+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "data = [\n",
    "    (1, \"Alice\", \"HR\", 50000),\n",
    "    (2, \"Bob\", \"IT\", None),  # NULL salary\n",
    "    (3, None, \"Finance\", 70000),  # NULL name\n",
    "    (4, \"Charlie\", None, 60000),  # NULL department\n",
    "    (5, \"David\", \"IT\", 65000),\n",
    "    (6, \"Emma\", \"HR\", None),  # NULL salary\n",
    "    (7, None, None, None),  # NULL name, department, salary\n",
    "]\n",
    "\n",
    "schema = StructType(\n",
    "    [\n",
    "        StructField(\"emp_id\", IntegerType(), nullable=True),\n",
    "        StructField(\"emp_name\", StringType(), nullable=True),\n",
    "        StructField(\"department\", StringType(), nullable=True),\n",
    "        StructField(\"salary\", IntegerType(), nullable=True),\n",
    "    ]\n",
    ")\n",
    "\n",
    "spark = local_spark_session()\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "\n",
    "def col_null_cnt(spark, input_df: DataFrame) -> DataFrame:\n",
    "    df = input_df.select(\n",
    "        [\n",
    "            sum(when(col(cl).isNull(), lit(1)).otherwise(lit(0))).alias(\n",
    "                f\"null_cnt_{cl}\"\n",
    "            )\n",
    "            for cl in input_df.columns\n",
    "        ]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "col_null_cnt(spark, df).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+--------------+----------+\n",
      "|cnt_emp_id|cnt_emp_name|cnt_department|cnt_salary|\n",
      "+----------+------------+--------------+----------+\n",
      "|         0|           2|             2|         3|\n",
      "+----------+------------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy().agg(\n",
    "    *[sum(col(cl).isNull().cast(\"int\")).alias(f\"cnt_{cl}\") for cl in df.columns]\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
